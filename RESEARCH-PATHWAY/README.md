# ğŸ”¬ AI-Augmented Research Pathway

**Transform your research workflow with AI orchestration**

From manual literature reviews â†’ Automated, systematic, scalable research systems

---

## ğŸ¯ What Is This?

A complete system for **academic researchers and scientists** to leverage AI for:

- ğŸ“š **Automated Literature Reviews** - Process 100+ papers in days, not months
- ğŸ¯ **Systematic Evaluation** - Custom rubrics for consistent quality assessment
- ğŸ§  **Intelligent Synthesis** - AI helps identify patterns, gaps, and insights
- âš¡ **Research Automation** - Scripts and workflows for repetitive tasks
- ğŸ”§ **AI Research Workbench** - Coordinated toolset for modern research

**This pathway starts from the same foundation** ([USER-PROFILE.md](../USER-PROFILE.md)) **but focuses entirely on research, not software development.**

---

## âš¡ Quick Start (For Researchers)

### 0. Create Your Research Profile (30 min) â­ **START HERE**

1. Complete main [USER-PROFILE.md](../USER-PROFILE.md)
2. Add research-specific sections: [RESEARCH-PROFILE-ADDITIONS.md](RESEARCH-PROFILE-ADDITIONS.md)
3. Save as `MY-RESEARCH-PROFILE.md`

### 1. Set Up AI Research Workbench (2-3 hours)

Follow [AI Workbench Setup Guide](WORKFLOWS/ai-workbench-setup.md):
- Install search tools (Scite.ai, Elicit, Consensus)
- Set up analysis AI (Claude Chat, ChatGPT)
- Configure citation management (Zotero)
- Organize note-taking system

### 2. Create Your Research Roadmap (15 min)

Use [Prompt R1: Research Roadmap Generator](PROMPTS/prompt-r1-research-roadmap.md):
- Input your research question and goals
- Get personalized structured learning path
- Tool recommendations for your domain
- Milestone projects to validate progress

### 3. Build Your Evaluation Framework (20 min)

Use [Prompt R2: Literature Evaluation Framework](PROMPTS/prompt-r2-literature-evaluation.md):
- Generate custom rubrics for your field
- Create screening criteria
- Build extraction templates
- Establish quality standards

### 4. Start Orchestrating (Ongoing)

Use [Prompt R3: Research Orchestration](PROMPTS/prompt-r3-research-orchestration.md):
- Coordinate multiple AI tools
- Run systematic literature reviews
- Automate paper discovery and evaluation
- Synthesize findings at scale

---

## ğŸ“š What's Inside?

### Core System
- **[ğŸ”¬ RESEARCH-PROFILE-ADDITIONS.md](RESEARCH-PROFILE-ADDITIONS.md)** - Research-specific profile template
- **[ğŸ¯ PROMPTS/](PROMPTS/)** - Three research prompts:
  - [Prompt R1: Research Roadmap Generator](PROMPTS/prompt-r1-research-roadmap.md)
  - [Prompt R2: Literature Evaluation Framework](PROMPTS/prompt-r2-literature-evaluation.md)
  - [Prompt R3: Research AI Orchestration](PROMPTS/prompt-r3-research-orchestration.md)

### Resources
- **[ğŸ”§ TOOLS/](TOOLS/)** - [Research AI Tool Matrix](TOOLS/research-ai-tool-matrix.csv) (Scite, Elicit, etc.)
- **[ğŸ“‹ WORKFLOWS/](WORKFLOWS/)** - [AI Workbench Setup](WORKFLOWS/ai-workbench-setup.md), systematic review workflows
- **[âš¡ QUICK-REFERENCE/](QUICK-REFERENCE/)** - Research quick guides
- **[ğŸŒ± DOMAIN-ADAPTATIONS/](DOMAIN-ADAPTATIONS/)** - Field-specific guides

---

## ğŸ¼ Research AI Orchestration Philosophy

**Traditional Research (Inefficient):**
```
You â†’ Google Scholar â†’ Read 100 papers manually â†’ Takes months
         â†“
    Overwhelmed, miss papers, inconsistent evaluation
```

**Orchestrated Research (Optimal):**
```
You (Conductor) â†’ Scite.ai (Citation analysis)
                â†’ Elicit (Semantic search)
                â†’ Perplexity (Current papers)
                â†’ Claude Chat (Deep analysis)
                â†’ Scripts (Automation)
                â†“
    Process 100+ papers systematically in weeks
```

### The Three Laws of Research AI Orchestration

1. **Law of Specialization** - Use Scite for citations, Claude for synthesis, Perplexity for current info
2. **Law of Context Transfer** - You carry research context between AIs (your profile, rubric, questions)
3. **Law of Human Judgment** - AI handles volume, you provide expertise and critical evaluation

---

## ğŸ”¬ The Research Trinity (3 Prompts)

### R1: Research Roadmap Generator
**When:** Starting new research area, need systematic approach
**AI:** Perplexity or Claude Chat
**Output:** Structured learning path for AI-augmented research

### R2: Literature Evaluation Framework
**When:** Need consistent paper assessment criteria
**AI:** Claude Chat or ChatGPT
**Output:** Custom rubrics, extraction templates, quality standards

### R3: Research AI Orchestration
**When:** Running literature reviews, managing high paper volume
**AI:** Multi-tool coordination (Scite + Claude + Perplexity + Scripts)
**Output:** Complete research pipeline from discovery â†’ synthesis

---

## ğŸ“Š Research Use Cases

### For PhD Students
- Literature review for dissertation
- Staying current in fast-moving fields
- Identifying research gaps systematically
- Grant proposal literature sections

### For Postdocs & Early Career
- Systematic reviews and meta-analyses
- Paper writing with strong lit reviews
- Grant writing with comprehensive backgrounds
- Peer review with systematic evaluation

### For Established Researchers
- Monitoring emerging research trends
- Interdisciplinary literature synthesis
- Large-scale evidence reviews
- Team research coordination

### For Industry Researchers
- Competitive intelligence
- Technology landscape analysis
- Evidence-based decision making
- Patent and publication monitoring

---

## ğŸ› ï¸ Research Tool Ecosystem

| Tool Type | Recommended Tools | Purpose |
|-----------|------------------|---------|
| **Search & Discovery** | Scite.ai, Elicit, Consensus, Google Scholar | Finding relevant papers |
| **Analysis & Synthesis** | Claude Chat, ChatGPT, Perplexity | Deep reading, synthesis, writing |
| **Citation Management** | Zotero, Mendeley | Organizing references |
| **Knowledge Management** | Notion, Obsidian, Spreadsheets | Notes and synthesis |
| **Visualization** | Research Rabbit, Connected Papers | Citation networks |
| **Automation** | Python scripts, Zapier, n8n | Monitoring, extraction |

See full comparison: [Research AI Tool Matrix](TOOLS/research-ai-tool-matrix.csv)

---

## ğŸ¯ Learning Path Overview

### Level 1: Manual Researcher (Current State)
- Uses Google Scholar only
- Reads papers one by one
- Takes scattered notes
- 10-20 papers = 1-2 weeks

### Level 2: AI-Assisted (Week 1-2)
- Uses 2-3 search tools
- AI helps with summaries
- Basic organization
- 20-30 papers = 1 week

### Level 3: Orchestrated Researcher â­ (Week 3-6)
**TARGET LEVEL - Most researchers stabilize here**
- Uses 5+ tools fluently
- Systematic evaluation with rubrics
- AI-human collaboration smooth
- 50-100 papers = 1-2 weeks

### Level 4: Automated Researcher (Week 7-8+)
- Full orchestration workflow
- Scripts for monitoring/extraction
- Teaching others the system
- 100+ papers = 1-2 weeks

---

## ğŸ’¡ Real-World Example: Dr. Chen's Transformation

**Before AI Orchestration:**
- Literature review for grant: 3 months
- Read 80 papers manually
- Missed key citations
- Synthesis felt overwhelming

**After (Week 8 of this system):**
- Same type of review: 3 weeks
- Processed 150 papers systematically
- Found papers Google Scholar missed
- Clear thematic synthesis
- **Result:** Grant funded, extra time for actual research

[See complete research journeys â†’](WORKFLOWS/research-success-stories.md)

---

## ğŸ“– Documentation Structure

```
RESEARCH-PATHWAY/
â”œâ”€â”€ README.md                              â† You are here
â”œâ”€â”€ RESEARCH-PROFILE-ADDITIONS.md          â† Add to main profile
â”‚
â”œâ”€â”€ PROMPTS/                               â† The Research Trinity
â”‚   â”œâ”€â”€ prompt-r1-research-roadmap.md
â”‚   â”œâ”€â”€ prompt-r2-literature-evaluation.md
â”‚   â””â”€â”€ prompt-r3-research-orchestration.md
â”‚
â”œâ”€â”€ WORKFLOWS/                             â† Step-by-step guides
â”‚   â”œâ”€â”€ ai-workbench-setup.md             â† Start here for setup
â”‚   â”œâ”€â”€ systematic-review-workflow.md
â”‚   â”œâ”€â”€ meta-analysis-workflow.md
â”‚   â””â”€â”€ paper-writing-workflow.md
â”‚
â”œâ”€â”€ TOOLS/                                 â† Resources
â”‚   â”œâ”€â”€ research-ai-tool-matrix.csv
â”‚   â”œâ”€â”€ research-scripts/                  â† Python examples
â”‚   â””â”€â”€ evaluation-rubric-templates/
â”‚
â”œâ”€â”€ QUICK-REFERENCE/                       â† Print these!
â”‚   â”œâ”€â”€ research-tool-selector.md
â”‚   â””â”€â”€ quality-checklist.md
â”‚
â””â”€â”€ DOMAIN-ADAPTATIONS/                    â† Field-specific
    â”œâ”€â”€ life-sciences.md
    â”œâ”€â”€ social-sciences.md
    â”œâ”€â”€ physical-sciences.md
    â””â”€â”€ humanities.md
```

---

## ğŸ¤ Contributing

Research-specific contributions welcome:

- ğŸ“š **Domain Adaptations** - Evaluation criteria for your field
- ğŸ”¬ **Workflow Patterns** - Your research orchestration patterns
- ğŸ“Š **Success Stories** - Share results and time savings
- ğŸ”§ **Tool Updates** - New research AI tools and integrations
- ğŸ“œ **Scripts** - Automation examples for common tasks

See [CONTRIBUTING.md](../CONTRIBUTING.md) for details.

---

## ğŸ“ Research Ethics & Quality

### Using AI Responsibly in Research

**Always:**
- âœ… Verify AI-generated citations (check they exist!)
- âœ… Deep-read key papers yourself
- âœ… Use AI for efficiency, not to skip thinking
- âœ… Be transparent about AI use in methods sections
- âœ… Maintain research rigor and critical evaluation

**Never:**
- âŒ Trust AI citations without verification
- âŒ Skip reading papers entirely
- âŒ Let AI make final judgments on quality
- âŒ Use AI-generated text without thorough editing
- âŒ Compromise research integrity for speed

### Quality Standards

**This system helps you:**
- Process MORE papers MORE thoroughly
- Find papers you'd otherwise miss
- Apply consistent evaluation criteria
- Identify patterns across large literature sets
- Free up time for deep thinking and analysis

**Goal:** Better research, faster. Never faster research, lower quality.

---

## ğŸ“œ License & Attribution

**License:** MIT - Share freely, adapt extensively

**Created by:** Magnus Smari Smarason
**Version:** 1.0 - Research Pathway
**Year:** 2025

**Part of:** [Taming Technology - AI Learning Orchestration System](../)

---

## ğŸš€ Ready to Start?

### Option 1: The Immediate Path (Start Now)
1. Complete [RESEARCH-PROFILE-ADDITIONS.md](RESEARCH-PROFILE-ADDITIONS.md)
2. Set up 2-3 tools from [Workbench Setup](WORKFLOWS/ai-workbench-setup.md)
3. Use [Prompt R1](PROMPTS/prompt-r1-research-roadmap.md) to get roadmap
4. Start with 10 papers to test workflow

### Option 2: The Prepared Path (Start Tomorrow)
1. Read complete guide
2. Set up full [AI Research Workbench](WORKFLOWS/ai-workbench-setup.md)
3. Create [evaluation rubric](PROMPTS/prompt-r2-literature-evaluation.md)
4. Follow [complete orchestration workflow](PROMPTS/prompt-r3-research-orchestration.md)

---

## ğŸ”— Related Pathways

- **[Development Pathway](../)** - For software development (not research)
- **[Main Guide](../COMPLETE-GUIDE.md)** - Full AI orchestration philosophy
- **[User Profile](../USER-PROFILE.md)** - Foundation for all pathways

---

## ğŸ“ Questions & Support

- **Research questions?** [Open a discussion](https://github.com/magnussmari/TamingTechnology/discussions)
- **Found an issue?** [Report it](https://github.com/magnussmari/TamingTechnology/issues)
- **Success story?** [Share it!](https://github.com/magnussmari/TamingTechnology/issues/new?template=success-story.md)

---

## ğŸ­ Remember

> "You're not just learning to use AI tools. You're learning to orchestrate intelligenceâ€”both artificial and your ownâ€”to advance knowledge that was previously out of reach."

**You have an entire orchestra of research AI tools ready to help.**
**You're the conductor.**
**This is your research symphony.**

**Now go advance knowledge.** ğŸ”¬ğŸ“šğŸ§ ğŸ“ŠğŸ“

---

**â­ Don't forget to star this repo if you found it helpful!**
