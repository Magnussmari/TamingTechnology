# 🔬 AI-Augmented Research Pathway

**Transform your research workflow with AI orchestration**

From manual literature reviews → Automated, systematic, scalable research systems

---

## 🎯 What Is This?

A complete system for **academic researchers and scientists** to leverage AI for:

- 📚 **Automated Literature Reviews** - Process 100+ papers in days, not months
- 🎯 **Systematic Evaluation** - Custom rubrics for consistent quality assessment
- 🧠 **Intelligent Synthesis** - AI helps identify patterns, gaps, and insights
- ⚡ **Research Automation** - Scripts and workflows for repetitive tasks
- 🔧 **AI Research Workbench** - Coordinated toolset for modern research

**This pathway starts from the same foundation** ([USER-PROFILE.md](../USER-PROFILE.md)) **but focuses entirely on research, not software development.**

---

## ⚡ Quick Start (For Researchers)

### 0. Create Your Research Profile (30 min) ⭐ **START HERE**

1. Complete main [USER-PROFILE.md](../USER-PROFILE.md)
2. Add research-specific sections: [RESEARCH-PROFILE-ADDITIONS.md](RESEARCH-PROFILE-ADDITIONS.md)
3. Save as `MY-RESEARCH-PROFILE.md`

### 1. Set Up AI Research Workbench (2-3 hours)

Follow [AI Workbench Setup Guide](WORKFLOWS/ai-workbench-setup.md):
- Install search tools (Scite.ai, Elicit, Consensus)
- Set up analysis AI (Claude Chat, ChatGPT)
- Configure citation management (Zotero)
- Organize note-taking system

### 2. Create Your Research Roadmap (15 min)

Use [Prompt R1: Research Roadmap Generator](PROMPTS/prompt-r1-research-roadmap.md):
- Input your research question and goals
- Get personalized structured learning path
- Tool recommendations for your domain
- Milestone projects to validate progress

### 3. Build Your Evaluation Framework (20 min)

Use [Prompt R2: Literature Evaluation Framework](PROMPTS/prompt-r2-literature-evaluation.md):
- Generate custom rubrics for your field
- Create screening criteria
- Build extraction templates
- Establish quality standards

### 4. Start Orchestrating (Ongoing)

Use [Prompt R3: Research Orchestration](PROMPTS/prompt-r3-research-orchestration.md):
- Coordinate multiple AI tools
- Run systematic literature reviews
- Automate paper discovery and evaluation
- Synthesize findings at scale

---

## 📚 What's Inside?

### Core System
- **[🔬 RESEARCH-PROFILE-ADDITIONS.md](RESEARCH-PROFILE-ADDITIONS.md)** - Research-specific profile template
- **[🎯 PROMPTS/](PROMPTS/)** - Three research prompts:
  - [Prompt R1: Research Roadmap Generator](PROMPTS/prompt-r1-research-roadmap.md)
  - [Prompt R2: Literature Evaluation Framework](PROMPTS/prompt-r2-literature-evaluation.md)
  - [Prompt R3: Research AI Orchestration](PROMPTS/prompt-r3-research-orchestration.md)

### Resources
- **[🔧 TOOLS/](TOOLS/)** - [Research AI Tool Matrix](TOOLS/research-ai-tool-matrix.csv) (Scite, Elicit, etc.)
- **[📋 WORKFLOWS/](WORKFLOWS/)** - [AI Workbench Setup](WORKFLOWS/ai-workbench-setup.md), systematic review workflows
- **[⚡ QUICK-REFERENCE/](QUICK-REFERENCE/)** - Research quick guides
- **[🌱 DOMAIN-ADAPTATIONS/](DOMAIN-ADAPTATIONS/)** - Field-specific guides

---

## 🎼 Research AI Orchestration Philosophy

**Traditional Research (Inefficient):**
```
You → Google Scholar → Read 100 papers manually → Takes months
         ↓
    Overwhelmed, miss papers, inconsistent evaluation
```

**Orchestrated Research (Optimal):**
```
You (Conductor) → Scite.ai (Citation analysis)
                → Elicit (Semantic search)
                → Perplexity (Current papers)
                → Claude Chat (Deep analysis)
                → Scripts (Automation)
                ↓
    Process 100+ papers systematically in weeks
```

### The Three Laws of Research AI Orchestration

1. **Law of Specialization** - Use Scite for citations, Claude for synthesis, Perplexity for current info
2. **Law of Context Transfer** - You carry research context between AIs (your profile, rubric, questions)
3. **Law of Human Judgment** - AI handles volume, you provide expertise and critical evaluation

---

## 🔬 The Research Trinity (3 Prompts)

### R1: Research Roadmap Generator
**When:** Starting new research area, need systematic approach
**AI:** Perplexity or Claude Chat
**Output:** Structured learning path for AI-augmented research

### R2: Literature Evaluation Framework
**When:** Need consistent paper assessment criteria
**AI:** Claude Chat or ChatGPT
**Output:** Custom rubrics, extraction templates, quality standards

### R3: Research AI Orchestration
**When:** Running literature reviews, managing high paper volume
**AI:** Multi-tool coordination (Scite + Claude + Perplexity + Scripts)
**Output:** Complete research pipeline from discovery → synthesis

---

## 📊 Research Use Cases

### For PhD Students
- Literature review for dissertation
- Staying current in fast-moving fields
- Identifying research gaps systematically
- Grant proposal literature sections

### For Postdocs & Early Career
- Systematic reviews and meta-analyses
- Paper writing with strong lit reviews
- Grant writing with comprehensive backgrounds
- Peer review with systematic evaluation

### For Established Researchers
- Monitoring emerging research trends
- Interdisciplinary literature synthesis
- Large-scale evidence reviews
- Team research coordination

### For Industry Researchers
- Competitive intelligence
- Technology landscape analysis
- Evidence-based decision making
- Patent and publication monitoring

---

## 🛠️ Research Tool Ecosystem

| Tool Type | Recommended Tools | Purpose |
|-----------|------------------|---------|
| **Search & Discovery** | Scite.ai, Elicit, Consensus, Google Scholar | Finding relevant papers |
| **Analysis & Synthesis** | Claude Chat, ChatGPT, Perplexity | Deep reading, synthesis, writing |
| **Citation Management** | Zotero, Mendeley | Organizing references |
| **Knowledge Management** | Notion, Obsidian, Spreadsheets | Notes and synthesis |
| **Visualization** | Research Rabbit, Connected Papers | Citation networks |
| **Automation** | Python scripts, Zapier, n8n | Monitoring, extraction |

See full comparison: [Research AI Tool Matrix](TOOLS/research-ai-tool-matrix.csv)

---

## 🎯 Learning Path Overview

### Level 1: Manual Researcher (Current State)
- Uses Google Scholar only
- Reads papers one by one
- Takes scattered notes
- 10-20 papers = 1-2 weeks

### Level 2: AI-Assisted (Week 1-2)
- Uses 2-3 search tools
- AI helps with summaries
- Basic organization
- 20-30 papers = 1 week

### Level 3: Orchestrated Researcher ⭐ (Week 3-6)
**TARGET LEVEL - Most researchers stabilize here**
- Uses 5+ tools fluently
- Systematic evaluation with rubrics
- AI-human collaboration smooth
- 50-100 papers = 1-2 weeks

### Level 4: Automated Researcher (Week 7-8+)
- Full orchestration workflow
- Scripts for monitoring/extraction
- Teaching others the system
- 100+ papers = 1-2 weeks

---

## 💡 Real-World Example: Dr. Chen's Transformation

**Before AI Orchestration:**
- Literature review for grant: 3 months
- Read 80 papers manually
- Missed key citations
- Synthesis felt overwhelming

**After (Week 8 of this system):**
- Same type of review: 3 weeks
- Processed 150 papers systematically
- Found papers Google Scholar missed
- Clear thematic synthesis
- **Result:** Grant funded, extra time for actual research

[See complete research journeys →](WORKFLOWS/research-success-stories.md)

---

## 📖 Documentation Structure

```
RESEARCH-PATHWAY/
├── README.md                              ← You are here
├── RESEARCH-PROFILE-ADDITIONS.md          ← Add to main profile
│
├── PROMPTS/                               ← The Research Trinity
│   ├── prompt-r1-research-roadmap.md
│   ├── prompt-r2-literature-evaluation.md
│   └── prompt-r3-research-orchestration.md
│
├── WORKFLOWS/                             ← Step-by-step guides
│   ├── ai-workbench-setup.md             ← Start here for setup
│   ├── systematic-review-workflow.md
│   ├── meta-analysis-workflow.md
│   └── paper-writing-workflow.md
│
├── TOOLS/                                 ← Resources
│   ├── research-ai-tool-matrix.csv
│   ├── research-scripts/                  ← Python examples
│   └── evaluation-rubric-templates/
│
├── QUICK-REFERENCE/                       ← Print these!
│   ├── research-tool-selector.md
│   └── quality-checklist.md
│
└── DOMAIN-ADAPTATIONS/                    ← Field-specific
    ├── life-sciences.md
    ├── social-sciences.md
    ├── physical-sciences.md
    └── humanities.md
```

---

## 🤝 Contributing

Research-specific contributions welcome:

- 📚 **Domain Adaptations** - Evaluation criteria for your field
- 🔬 **Workflow Patterns** - Your research orchestration patterns
- 📊 **Success Stories** - Share results and time savings
- 🔧 **Tool Updates** - New research AI tools and integrations
- 📜 **Scripts** - Automation examples for common tasks

See [CONTRIBUTING.md](../CONTRIBUTING.md) for details.

---

## 🎓 Research Ethics & Quality

### Using AI Responsibly in Research

**Always:**
- ✅ Verify AI-generated citations (check they exist!)
- ✅ Deep-read key papers yourself
- ✅ Use AI for efficiency, not to skip thinking
- ✅ Be transparent about AI use in methods sections
- ✅ Maintain research rigor and critical evaluation

**Never:**
- ❌ Trust AI citations without verification
- ❌ Skip reading papers entirely
- ❌ Let AI make final judgments on quality
- ❌ Use AI-generated text without thorough editing
- ❌ Compromise research integrity for speed

### Quality Standards

**This system helps you:**
- Process MORE papers MORE thoroughly
- Find papers you'd otherwise miss
- Apply consistent evaluation criteria
- Identify patterns across large literature sets
- Free up time for deep thinking and analysis

**Goal:** Better research, faster. Never faster research, lower quality.

---

## 📜 License & Attribution

**License:** MIT - Share freely, adapt extensively

**Created by:** Magnus Smari Smarason
**Version:** 1.0 - Research Pathway
**Year:** 2025

**Part of:** [Taming Technology - AI Learning Orchestration System](../)

---

## 🚀 Ready to Start?

### Option 1: The Immediate Path (Start Now)
1. Complete [RESEARCH-PROFILE-ADDITIONS.md](RESEARCH-PROFILE-ADDITIONS.md)
2. Set up 2-3 tools from [Workbench Setup](WORKFLOWS/ai-workbench-setup.md)
3. Use [Prompt R1](PROMPTS/prompt-r1-research-roadmap.md) to get roadmap
4. Start with 10 papers to test workflow

### Option 2: The Prepared Path (Start Tomorrow)
1. Read complete guide
2. Set up full [AI Research Workbench](WORKFLOWS/ai-workbench-setup.md)
3. Create [evaluation rubric](PROMPTS/prompt-r2-literature-evaluation.md)
4. Follow [complete orchestration workflow](PROMPTS/prompt-r3-research-orchestration.md)

---

## 🔗 Related Pathways

- **[Development Pathway](../)** - For software development (not research)
- **[Main Guide](../COMPLETE-GUIDE.md)** - Full AI orchestration philosophy
- **[User Profile](../USER-PROFILE.md)** - Foundation for all pathways

---

## 📞 Questions & Support

- **Research questions?** [Open a discussion](https://github.com/magnussmari/TamingTechnology/discussions)
- **Found an issue?** [Report it](https://github.com/magnussmari/TamingTechnology/issues)
- **Success story?** [Share it!](https://github.com/magnussmari/TamingTechnology/issues/new?template=success-story.md)

---

## 🎭 Remember

> "You're not just learning to use AI tools. You're learning to orchestrate intelligence—both artificial and your own—to advance knowledge that was previously out of reach."

**You have an entire orchestra of research AI tools ready to help.**
**You're the conductor.**
**This is your research symphony.**

**Now go advance knowledge.** 🔬📚🧠📊🎓

---

**⭐ Don't forget to star this repo if you found it helpful!**
